{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# # download nltk corpus\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "corpus = brown.sents()\n",
    "# select only the first 1000 stories\n",
    "corpus = corpus[:1000]\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numeralization\n",
    "\n",
    "### find unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>\n",
    "vocabs.append('<UNK>') #append unknown token to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 2 words\n",
    "            outside = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                outside.append(word2index[doc[j]])\n",
    "            #for each of these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                #center, outside1;   center, outside2\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Negative Sampling\n",
    "\n",
    "#### Unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22079"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "word_count\n",
    "\n",
    "#get the total number of words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if torch.cuda.is_available():  \n",
    "#   dev = \"cuda:0\" \n",
    "# else:  \n",
    "#   dev = \"cpu\"  \n",
    "# device = torch.device(dev) \n",
    "device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "atch_size = 2\n",
    "emb_size   = 2\n",
    "batch_size = 2\n",
    "window_size = 2\n",
    "voc_size   = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ..., 4270, 4271, 4272],\n",
       "        [   0,    1,    2,  ..., 4270, 4271, 4272]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size).to(device)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skipgram Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Neo\\anaconda3\\envs\\dsai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      1 | Loss: 9.164570\n",
      "Epoch      2 | Loss: 10.149135\n",
      "Epoch      3 | Loss: 7.733615\n",
      "Epoch      4 | Loss: 8.794529\n",
      "Epoch      5 | Loss: 9.012422\n",
      "Epoch      6 | Loss: 8.760987\n",
      "Epoch      7 | Loss: 8.126495\n",
      "Epoch      8 | Loss: 9.138553\n",
      "Epoch      9 | Loss: 8.062308\n",
      "Epoch     10 | Loss: 11.005460\n",
      "Training time: 3.1809864044189453\n"
     ]
    }
   ],
   "source": [
    "skipgram_model  = Skipgram(voc_size, emb_size).to(device)\n",
    "optimizer  = optim.Adam(skipgram_model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "    \n",
    "    #predict\n",
    "    loss = skipgram_model(input_tensor, label_tensor, all_vocabs)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")\n",
    "\n",
    "print(f\"Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neg Sampling Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch      1 | Loss: 1.934021\n",
      "Epoch      2 | Loss: 2.607116\n",
      "Epoch      3 | Loss: 1.133654\n",
      "Epoch      4 | Loss: 2.830342\n",
      "Epoch      5 | Loss: 2.534127\n",
      "Epoch      6 | Loss: 1.554465\n",
      "Epoch      7 | Loss: 1.084432\n",
      "Epoch      8 | Loss: 1.945701\n",
      "Epoch      9 | Loss: 0.972715\n",
      "Epoch     10 | Loss: 1.252883\n",
      "Training time: 2.999988555908203\n"
     ]
    }
   ],
   "source": [
    "neg_model   = SkipgramNeg(voc_size, emb_size).to(device)\n",
    "optimizer  = optim.Adam(neg_model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "k = 5\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "    \n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k).to(device)\n",
    "    loss = neg_model(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")\n",
    "\n",
    "print(f\"Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build Co-occurence Matrix X\n",
    "\n",
    "Here, we need to count the co-occurence of two words given some window size.  We gonna use window size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "X_i = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for i in range(2, len(doc)-2):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-2], doc[i-1], doc[i+1], doc[i+2]]\n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighting function\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurences between w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        #if not exist, then set to 1 \"laplace smoothing\"\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #keeping the co-occurences\n",
    "weighting_dic = {} #already scale the co-occurences using the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):\n",
    "    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n",
    "        co = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram] = co + 1 #for stability\n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 #basically apple, banana = banana, apple\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    #convert our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly choose indexes based on batch size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n",
    "    \n",
    "    #get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "        #coocs\n",
    "        pair = skip_grams[index] #e.g., ('banana', 'fruit')\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "    \n",
    "        #weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "glove_model = Glove(voc_size, embedding_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(glove_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | cost: 95.865410\n",
      "Epoch: 2 | cost: 14.577272\n",
      "Epoch: 3 | cost: 12.333422\n",
      "Epoch: 4 | cost: 2.934718\n",
      "Epoch: 5 | cost: 9.889151\n",
      "Epoch: 6 | cost: 13.056210\n",
      "Epoch: 7 | cost: 44.128540\n",
      "Epoch: 8 | cost: 4.856674\n",
      "Epoch: 9 | cost: 10.214202\n",
      "Epoch: 10 | cost: 2.058057\n",
      "Training time: 59.404550552368164\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch).to(device)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch).to(device)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch).to(device)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch).to(device) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = glove_model(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1} | cost: {loss:.6f}\")\n",
    "    \n",
    "print(f\"Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath(os.path.abspath('glove.6B.100d.txt'))  #search on the google\n",
    "gensim_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic and Syntatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model, vocabs):\n",
    "    embeds = {}\n",
    "\n",
    "    for word in vocabs:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except:\n",
    "            index = word2index['<UNK>']\n",
    "            \n",
    "        word_idx = torch.LongTensor([word2index[word]])\n",
    "        \n",
    "        embed_c = model.embedding_center(word_idx)\n",
    "        embed_o = model.embedding_outside(word_idx)\n",
    "        embed   = (embed_c + embed_o) / 2\n",
    "        embed = embed[0][0].item(), embed[0][1].item()\n",
    "        embeds[word] = np.array(embed)\n",
    "    \n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(embeddings, word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        word = '<UNK>'\n",
    "    \n",
    "    return embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the embeddings from each of our model\n",
    "skipgram_embeds = compute_embeddings(skipgram_model, vocabs)\n",
    "neg_embeds = compute_embeddings(neg_model, vocabs)\n",
    "glove_embeds = compute_embeddings(glove_model, vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_dict = {\n",
    "    \"skipgram_embeds\": skipgram_embeds,\n",
    "    \"neg_embeds\": neg_embeds,\n",
    "    \"glove_embeds\": glove_embeds\n",
    "}\n",
    "\n",
    "for embeds in embeds_dict.items():\n",
    "    with open(f\"app/embeddings/{embeds[0]}.pickle\", \"wb\") as f:\n",
    "        pickle.dump(embeds[1], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28234041,  0.20191008])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed(neg_embeds, 'greece')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the analogy dataset\n",
    "with open(\"word-test.v1.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "data = data.replace(\"\\t\", \"\")\n",
    "# split the dataset based on their categories\n",
    "analogy = data.split(': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       " ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       " ['Athens', 'Greece', 'Beijing', 'China'],\n",
       " ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       " ['Athens', 'Greece', 'Bern', 'Switzerland']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the 'capital-common-countries' section of the dataset\n",
    "capital = analogy[1].split('\\n')[1:-1]\n",
    "capital = [x.split(\" \") for x in capital]\n",
    "capital[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dancing', 'danced', 'decreasing', 'decreased'],\n",
       " ['dancing', 'danced', 'describing', 'described'],\n",
       " ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       " ['dancing', 'danced', 'falling', 'fell'],\n",
       " ['dancing', 'danced', 'feeding', 'fed']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the 'gram7-past-tense' section of the dataset\n",
    "past_tense = analogy[12].split('\\n')[1:-1]\n",
    "past_tense = [x.split(\" \") for x in past_tense]\n",
    "past_tense[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Athens', 'Greece', 'Bangkok', 'Thailand']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28234041,  0.20191008])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Greece - Athens + Bangkok\n",
    "# ground-truth == y_true == 'Thailand'\n",
    "i = 1\n",
    "y_pred = get_embed(neg_embeds, capital[i][1].lower()) - get_embed(neg_embeds, capital[i][0].lower()) + get_embed(neg_embeds, capital[i][2].lower())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the most similar word to the input vector\n",
    "def get_most_similar(vector, embeddings):\n",
    "    # retrieve all words in our embeddings vocabs\n",
    "    try:\n",
    "        words = list(embeddings.keys())\n",
    "    except:\n",
    "        words = list(embeddings.key_to_index.keys())\n",
    "    \n",
    "    similarities = {}\n",
    "\n",
    "    # for each word in the vocabs, find the cosine similarities between word vectors in our embeddings and the input vector\n",
    "    for word in words:\n",
    "        similarities[word] = cosine_similarity(vector, embeddings[word])\n",
    "\n",
    "    # return the word with the most similar vector to the input vector\n",
    "    return max(similarities, key=similarities.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the most similar word to the input vector\n",
    "def cosine_ranking(vector, embeddings):\n",
    "    # retrieve all words in our embeddings vocabs\n",
    "    try:\n",
    "        words = list(embeddings.keys())\n",
    "    except:\n",
    "        words = list(embeddings.key_to_index.keys())\n",
    "    \n",
    "    similarities = {}\n",
    "\n",
    "    # for each word in the vocabs, find the cosine similarities between word vectors in our embeddings and the input vector\n",
    "    for word in words:\n",
    "        similarities[word] = cosine_similarity(vector, embeddings[word])\n",
    "\n",
    "    # return the word with the most similar vector to the input vector\n",
    "    # return similarities\n",
    "    return dict(sorted(similarities.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding semantic and syntactic accuracies\n",
    "def find_accuracy(dataset, embeddings):\n",
    "    matched_count = 0\n",
    "\n",
    "    for data in dataset:\n",
    "        row = [word.lower() for word in data]\n",
    "        \n",
    "        # find the predicted vector\n",
    "        try:\n",
    "            pred_y = get_embed(embeddings, row[1]) - get_embed(embeddings, row[0]) + get_embed(embeddings, row[2])\n",
    "            pred_word = get_most_similar(pred_y, embeddings)\n",
    "        except:\n",
    "            pred_word = embeddings.most_similar(positive=[row[1], row[2]], negative=[row[0]])[0][0]\n",
    "\n",
    "        # if the ground-truth word matched with the word where its corresponding vector are the closest to the predicted vector, increase matched_count\n",
    "        if row[3] == pred_word:\n",
    "            matched_count += 1\n",
    "\n",
    "    # count of matched / count of all as accuracy\n",
    "    return matched_count / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_sem = find_accuracy(capital, skipgram_embeds)\n",
    "skipgram_syn = find_accuracy(past_tense, skipgram_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sem = find_accuracy(capital, neg_embeds)\n",
    "neg_syn = find_accuracy(past_tense, neg_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_sem = find_accuracy(capital, glove_embeds)\n",
    "glove_syn = find_accuracy(past_tense, glove_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_sem = find_accuracy(capital, gensim_model)\n",
    "gensim_syn = find_accuracy(past_tense, gensim_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Word2Vec (Skipgram) ===\n",
      "Semantic accuracy: 0.0\n",
      "Syntatic accuracy: 0.000641025641025641\n",
      "\n",
      "=== Word2Vec (Negative Sampling) ===\n",
      "Semantic accuracy: 0.0\n",
      "Syntatic accuracy: 0.0\n",
      "\n",
      "=== GloVe from Scratch ===\n",
      "Semantic accuracy: 0.001976284584980237\n",
      "Syntatic accuracy: 0.0\n",
      "\n",
      "=== GloVe (Gensim) ===\n",
      "Semantic accuracy: 0.9387351778656127\n",
      "Syntatic accuracy: 0.5064102564102564\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Word2Vec (Skipgram) ===\")\n",
    "print(f\"Semantic accuracy: {skipgram_sem}\")\n",
    "print(f\"Syntatic accuracy: {skipgram_syn}\\n\")\n",
    "\n",
    "print(\"=== Word2Vec (Negative Sampling) ===\")\n",
    "print(f\"Semantic accuracy: {neg_sem}\")\n",
    "print(f\"Syntatic accuracy: {neg_syn}\\n\")\n",
    "\n",
    "print(\"=== GloVe from Scratch ===\")\n",
    "print(f\"Semantic accuracy: {glove_sem}\")\n",
    "print(f\"Syntatic accuracy: {glove_syn}\\n\")\n",
    "\n",
    "print(\"=== GloVe (Gensim) ===\")\n",
    "print(f\"Semantic accuracy: {gensim_sem}\")\n",
    "print(f\"Syntatic accuracy: {gensim_syn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarities\n",
       "0         tiger       cat          7.35\n",
       "1         tiger     tiger         10.00\n",
       "2         plane       car          5.77\n",
       "3         train       car          6.31\n",
       "4    television     radio          6.77\n",
       "..          ...       ...           ...\n",
       "198     rooster    voyage          0.62\n",
       "199        noon    string          0.54\n",
       "200       chord     smile          0.54\n",
       "201   professor  cucumber          0.31\n",
       "202        king   cabbage          0.23\n",
       "\n",
       "[203 rows x 3 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load word similarity dataset as pandas dataframe\n",
    "wordsim = pd.read_csv('wordsim_similarity_goldstandard.txt', sep=\"\\t\", header=None, names=['word_1', 'word_2', 'similarities'])\n",
    "wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarities</th>\n",
       "      <th>SKIP_dot_product</th>\n",
       "      <th>NEG_dot_product</th>\n",
       "      <th>glove_dot_product</th>\n",
       "      <th>gensim_dot_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>4.754888</td>\n",
       "      <td>0.352016</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>15.629377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>4.754888</td>\n",
       "      <td>0.352016</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>32.800144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>-0.630410</td>\n",
       "      <td>-0.569838</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>24.047297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-0.630410</td>\n",
       "      <td>-0.569838</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>25.472925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>-0.130191</td>\n",
       "      <td>-0.196611</td>\n",
       "      <td>0.097703</td>\n",
       "      <td>34.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>4.754888</td>\n",
       "      <td>0.352016</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>1.683646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-3.843663</td>\n",
       "      <td>-0.828806</td>\n",
       "      <td>0.298204</td>\n",
       "      <td>1.070593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>4.754888</td>\n",
       "      <td>0.352016</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>6.762520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>2.144881</td>\n",
       "      <td>0.626350</td>\n",
       "      <td>0.348931</td>\n",
       "      <td>-0.230552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>4.754888</td>\n",
       "      <td>0.352016</td>\n",
       "      <td>0.432315</td>\n",
       "      <td>1.400288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarities  SKIP_dot_product  NEG_dot_product  \\\n",
       "0         tiger       cat          7.35          4.754888         0.352016   \n",
       "1         tiger     tiger         10.00          4.754888         0.352016   \n",
       "2         plane       car          5.77         -0.630410        -0.569838   \n",
       "3         train       car          6.31         -0.630410        -0.569838   \n",
       "4    television     radio          6.77         -0.130191        -0.196611   \n",
       "..          ...       ...           ...               ...              ...   \n",
       "198     rooster    voyage          0.62          4.754888         0.352016   \n",
       "199        noon    string          0.54         -3.843663        -0.828806   \n",
       "200       chord     smile          0.54          4.754888         0.352016   \n",
       "201   professor  cucumber          0.31          2.144881         0.626350   \n",
       "202        king   cabbage          0.23          4.754888         0.352016   \n",
       "\n",
       "     glove_dot_product  gensim_dot_product  \n",
       "0             0.432315           15.629377  \n",
       "1             0.432315           32.800144  \n",
       "2             0.432315           24.047297  \n",
       "3             0.432315           25.472925  \n",
       "4             0.097703           34.689987  \n",
       "..                 ...                 ...  \n",
       "198           0.432315            1.683646  \n",
       "199           0.298204            1.070593  \n",
       "200           0.432315            6.762520  \n",
       "201           0.348931           -0.230552  \n",
       "202           0.432315            1.400288  \n",
       "\n",
       "[203 rows x 7 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim['SKIP_dot_product'] = wordsim.apply(lambda row: np.dot(\n",
    "    get_embed(skipgram_embeds, row['word_1'].lower()), get_embed(skipgram_embeds, row['word_2'].lower())\n",
    "    ), axis=1)\n",
    "\n",
    "wordsim['NEG_dot_product'] = wordsim.apply(lambda row: np.dot(\n",
    "    get_embed(neg_embeds, row['word_1'].lower()), get_embed(neg_embeds, row['word_2'].lower())\n",
    "    ), axis=1)\n",
    "\n",
    "wordsim['glove_dot_product'] = wordsim.apply(lambda row: np.dot(\n",
    "    get_embed(glove_embeds, row['word_1'].lower()), get_embed(glove_embeds, row['word_1'].lower())\n",
    "    ), axis=1)\n",
    "\n",
    "wordsim['gensim_dot_product'] = wordsim.apply(lambda row: np.dot(\n",
    "    gensim_model[row['word_1'].lower()], gensim_model[row['word_2'].lower()]\n",
    "    ), axis=1)\n",
    "\n",
    "wordsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spearman correlations ===\n",
      "Word2Vec (Skipgram): 0.0818425236423009\n",
      "Word2Vec (Negative Sampling): 0.09041614439553024\n",
      "GloVe from Scratch: 0.03145212607529534\n",
      "GloVe (Gensim): 0.5430870624672256\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# finding spearman correlations between wordsim353 similarities and our embeddings dot products\n",
    "wordsim_sim = wordsim['similarities'].to_numpy()\n",
    "skipgram_sim = wordsim['SKIP_dot_product'].to_numpy()\n",
    "neg_sim = wordsim['NEG_dot_product'].to_numpy()\n",
    "glove_sim = wordsim['glove_dot_product'].to_numpy()\n",
    "gensim_sim = wordsim['gensim_dot_product'].to_numpy()\n",
    "\n",
    "print(\"=== Spearman correlations ===\")\n",
    "print(f\"Word2Vec (Skipgram): {spearmanr(wordsim_sim, skipgram_sim).statistic}\")\n",
    "print(f\"Word2Vec (Negative Sampling): {spearmanr(wordsim_sim, neg_sim).statistic}\")\n",
    "print(f\"GloVe from Scratch: {spearmanr(wordsim_sim, glove_sim).statistic}\")\n",
    "print(f\"GloVe (Gensim): {spearmanr(wordsim_sim, gensim_sim).statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model Accuracies and Training Time Comparison</h4>\n",
    "\n",
    "| **Model**          | **Window Size** | **Training Loss** | **Training time** | **Syntactic Accuracy** | **Semantic accuracy** |\n",
    "|--------------------|:---------------:|:-----------------:|:-----------------:|:----------------------:|:---------------------:|\n",
    "| **Skipgram**       |        2        |      11.0055      |       3.18 s      |          0.06%         |           0%          |\n",
    "| **Skipgram (NEG)** |        2        |       1.2529      |       2.99 s      |           0%           |           0%          |\n",
    "| **GloVe**          |        2        |       2.0581      |      59.40 s      |           0%           |          0.2%         |\n",
    "| **GloVe (Gensim)** |        -        |         -         |         -         |         50.64%         |         93.87%        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Correlation between Model Dot Product and Score by Human Judgement</h4>\n",
    "\n",
    "| **Model**                | **Skipgram** | **NEG** | **GloVe** | **GloVe (gensim)** |\n",
    "|--------------------------|--------------|---------|-----------|--------------------|\n",
    "| **Spearman Correlation** |    0.0818    |  0.0904 |   0.0315  |       0.5431       |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
